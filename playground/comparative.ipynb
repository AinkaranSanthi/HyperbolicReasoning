{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c341d1d-f296-46fc-926c-86c69edb442e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib\n",
    "import IPython.display\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "sys.path.append('..')\n",
    "from src.train import Trainer\n",
    "from src.utils import set_requires_grad\n",
    "from clsmodel import mnist, stl10, afhq\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.comparative import LIME, SHAP, GradCAM, HyperbolicReasoning, DeepLIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b69c93-fa10-46e7-9597-3a8e5df7b6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load exp config\n",
    "model_path = './LOGS/MNIST/models/best.pth'\n",
    "\n",
    "#data-folder-path\n",
    "folder_path = '../datasets/MNIST'\n",
    "\n",
    "config = json.load(open(os.path.join(os.path.dirname(os.path.dirname(model_path)), 'exp-config.json'), 'r'))\n",
    "\n",
    "# config['classifier'] = afhq(32, True).cuda(0)\n",
    "config['classifier'] = mnist(32, 'it').cuda(0)\n",
    "\n",
    "model = Trainer(**config)\n",
    "model.load_model(model_path)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# model defns\n",
    "feature_extractor = model.feature_extractor\n",
    "conti_classifier = model.classifier_baseline\n",
    "dis_classifier = model.classifier_quantized\n",
    "decoder = model.dec\n",
    "codebook_sampler = model.modelclass\n",
    "\n",
    "\n",
    "# set_requires_grad(classifier, False)\n",
    "set_requires_grad(feature_extractor, False)\n",
    "set_requires_grad(conti_classifier, False)\n",
    "set_requires_grad(dis_classifier, False)\n",
    "set_requires_grad(decoder, False)\n",
    "set_requires_grad(codebook_sampler, False)\n",
    "    \n",
    "\n",
    "# required config variables \n",
    "image_size = model.input_size\n",
    "latent_dim = model.latent_size\n",
    "num_classes = model.nclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d0b265-37c9-4540-979c-94a21af07fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import get\n",
    "\n",
    "batch_size = 16\n",
    "train_loader, test_loader = get(batch_size, \n",
    "                      data_root = folder_path, \n",
    "                      train=True, val=True, \n",
    "                      input_size = image_size,\n",
    "                      num_workers=10)\n",
    "\n",
    "\n",
    "orig_images = []; orig_labels = []\n",
    "for (img_batch, label_batch, *context_batch) in tqdm(test_loader):\n",
    "    img_batch = img_batch.cuda(0)\n",
    "    label_batch = label_batch.cuda(0)\n",
    "    orig_images.extend(img_batch)\n",
    "    orig_labels.extend(label_batch)\n",
    "\n",
    "\n",
    "labels_ = np.array([lb.cpu().numpy() for lb in orig_labels])\n",
    "# generate sample dataset\n",
    "def get_sample_data(N_datapoints = 10): \n",
    "    sample_images = []; sample_labels = []\n",
    "    for ci in range(num_classes):\n",
    "        idxs = np.where(labels_ == ci)[0]\n",
    "        randidxs = idxs[np.random.randint(0, len(idxs), N_datapoints//num_classes)]\n",
    "\n",
    "        for idx in randidxs:\n",
    "            sample_images.append(orig_images[idx].unsqueeze(0))\n",
    "            sample_labels.append(orig_labels[idx].unsqueeze(0))\n",
    "\n",
    "    sample_images = torch.cat(sample_images, dim = 0)\n",
    "    sample_labels = torch.cat(sample_labels, dim = 0)\n",
    "    return sample_images, sample_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61609322-fdee-431b-9fb1-48498df657ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference import InductiveReasoningDT\n",
    "\n",
    "inferer = InductiveReasoningDT(get_sample_data(100),\n",
    "                               ncodebook_features = model.codebook_size,\n",
    "                               nclasses = num_classes,\n",
    "                               train_loader = train_loader,\n",
    "                               feature_extractor = feature_extractor,\n",
    "                               codebook = codebook_sampler,\n",
    "                               classifier = dis_classifier,\n",
    "                               decoder = decoder)\n",
    "\n",
    "for ci in range(num_classes):\n",
    "    inferer.get_class_tree(ci,f'./MNISTLOGS/class-{ci}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b119e0-7ea9-4f4f-af34-b5f50226efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init all explainers\n",
    "import torch.nn as nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_extractor, classifier):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.classifier = classifier\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = F.adaptive_avg_pool2d(x,(1,1)).view(x.size(0),-1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "classifier = Classifier(feature_extractor, classifier)\n",
    "lime = LIME(classifier)\n",
    "shap = SHAP(classifier)\n",
    "deeplift = DeepLIFT(classifier)\n",
    "gradcam = GradCAM(classifier, layeridx=-1)\n",
    "hyperbolic = HyperbolicReasoning(inferer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1ebf7e-ea6c-4af8-bacb-10a326e6f9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images, sample_labels = get_sample_data(10)\n",
    "\n",
    "explainers = {'LIME': lime, 'SHAP':shap, 'deeplift': deeplift, 'gradcam': gradcam, 'hyperbolic' hyperbolic}\n",
    "\n",
    "noise_weightage = [0.01, 0.1, 0.2, 0.3]\n",
    "noise_type = ['gauss', 's&p', 'poisson', 'speckle']\n",
    "\n",
    "for expname, exp in explainer.items():\n",
    "    for nw in noise_weightage:\n",
    "        for ny in noise_type:\n",
    "            exp_robustness = exp.compute_robustness(sample_images, \n",
    "                                                    sample_labels,\n",
    "                                                    type = ny, \n",
    "                                                    percentage = nw)\n",
    "            print (f\"Explainer Type: {expname}, Noise Type: {ny}, Noise Weightage: {nw}, Robustness: {exp_robustness}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
